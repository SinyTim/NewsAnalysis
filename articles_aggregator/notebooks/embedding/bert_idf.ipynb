{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.common.file import read_json\n",
    "from deeppavlov import build_model\n",
    "from deeppavlov import configs\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pymorphy2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus_tut = Path('../data/corpora/tutby_126784.csv')\n",
    "\n",
    "path_model = Path('../data/model/bert/rubert_cased_L-12_H-768_A-12_pt')\n",
    "\n",
    "path_emb = Path('../data/emb/tutby_126784_doc_rubert_token')\n",
    "path_tokens = Path('../data/emb/tutby_126784_doc_rubert_pymorphy2.pickle')\n",
    "path_embw = Path('../data/emb/tutby_126784_doc_rubert_tokenw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>header</th>\n",
       "      <th>date</th>\n",
       "      <th>document</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://news.tut.by/550306.html</td>\n",
       "      <td>Футбол</td>\n",
       "      <td>Тренер \"Шахтера\": Оправдываться не хочу. Все в...</td>\n",
       "      <td>2017-07-06T21:35:00+03:00</td>\n",
       "      <td>Главный тренер солигорского «Шахтера» Олег Куб...</td>\n",
       "      <td>['футбол']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://news.tut.by/550307.html</td>\n",
       "      <td>Общество</td>\n",
       "      <td>\"Зацветет\" ли каменная роза на ул. Комсомольск...</td>\n",
       "      <td>2017-07-07T09:25:00+03:00</td>\n",
       "      <td>Планы по восстановлению рисунка есть. Но пока ...</td>\n",
       "      <td>['архитектура', 'живопись', 'ЖКХ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://news.tut.by/550308.html</td>\n",
       "      <td>Общество</td>\n",
       "      <td>Фотофакт. Скамейка в виде пожарной машины появ...</td>\n",
       "      <td>2017-07-07T09:27:00+03:00</td>\n",
       "      <td>Областное управление МЧС ко Дню пожарной служб...</td>\n",
       "      <td>['министерства']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url     label  \\\n",
       "0  https://news.tut.by/550306.html    Футбол   \n",
       "1  https://news.tut.by/550307.html  Общество   \n",
       "2  https://news.tut.by/550308.html  Общество   \n",
       "\n",
       "                                              header  \\\n",
       "0  Тренер \"Шахтера\": Оправдываться не хочу. Все в...   \n",
       "1  \"Зацветет\" ли каменная роза на ул. Комсомольск...   \n",
       "2  Фотофакт. Скамейка в виде пожарной машины появ...   \n",
       "\n",
       "                        date  \\\n",
       "0  2017-07-06T21:35:00+03:00   \n",
       "1  2017-07-07T09:25:00+03:00   \n",
       "2  2017-07-07T09:27:00+03:00   \n",
       "\n",
       "                                            document  \\\n",
       "0  Главный тренер солигорского «Шахтера» Олег Куб...   \n",
       "1  Планы по восстановлению рисунка есть. Но пока ...   \n",
       "2  Областное управление МЧС ко Дню пожарной служб...   \n",
       "\n",
       "                                 tags  \n",
       "0                          ['футбол']  \n",
       "1  ['архитектура', 'живопись', 'ЖКХ']  \n",
       "2                    ['министерства']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(path_corpus_tut)\n",
    "\n",
    "corpus = data['document'].fillna('')\n",
    "# corpus = data['header']\n",
    "\n",
    "corpus = corpus.str.slice(0, 1000)\n",
    "\n",
    "corpus = corpus.tolist()\n",
    "print(len(corpus))\n",
    "display(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_config = read_json(configs.embedder.bert_embedder)\n",
    "bert_config['metadata']['variables']['BERT_PATH'] = path_model\n",
    "\n",
    "model = build_model(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-08 23:35:52.475955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fd0c42d74f4ceb86ce5c85b7af9207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 5h 18min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now())\n",
    "\n",
    "batch_size = 16  # 256\n",
    "n_batches = len(corpus) // batch_size + int(len(corpus) % batch_size != 0)\n",
    "\n",
    "for i in tqdm(range(n_batches)):\n",
    "    batch = corpus[batch_size * i : batch_size * (i + 1)]\n",
    "    tokens_batch, token_embs, _, _, _, sent_mean_embs, _ = model(batch)\n",
    "    \n",
    "    path = path_emb / f'batch_{i}.pickle'\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump((tokens_batch, token_embs, sent_mean_embs), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 04:54:37.546563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b771669b79466c9938e3fc271212b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7924\n",
      "Wall time: 1h 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.now())\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "tokens_lemmatized = []\n",
    "\n",
    "paths = list(path_emb.iterdir())\n",
    "\n",
    "for path_batch in tqdm(paths):\n",
    "    with open(path_batch, 'rb') as file:\n",
    "        tokens_batch, _, _ = pickle.load(file)\n",
    "\n",
    "    lemmatized = [[morph.parse(token)[0].normal_form for token in tokens_] for tokens_ in tokens_batch]\n",
    "    \n",
    "    tokens_lemmatized += [(path_batch, lemmatized)]\n",
    "\n",
    "    \n",
    "with open(path_tokens, 'wb') as file:\n",
    "    pickle.dump(tokens_lemmatized, file)\n",
    "    \n",
    "with open(path_tokens, 'rb') as file:\n",
    "    tokens_lemmatized = pickle.load(file)\n",
    "    \n",
    "print(len(tokens_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215526,) 215526\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens_lemmatized_concat = [' '.join(tt) for _, t in tokens_lemmatized for tt in t]\n",
    "\n",
    "vectorizer_idf = TfidfVectorizer(norm='l1', use_idf=True)\n",
    "vectorizer_idf.fit(tokens_lemmatized_concat)\n",
    "\n",
    "idf = vectorizer_idf.idf_\n",
    "idf = idf / idf.sum()\n",
    "vocabulary = vectorizer_idf.get_feature_names()\n",
    "word2idf = dict(zip(vocabulary, idf))\n",
    "\n",
    "print(idf.shape, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c8817b4bf94ec19a72adeea60c1f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(126784, 768)\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_w = []\n",
    "\n",
    "for path_batch, words_ in tqdm(tokens_lemmatized):\n",
    "    with open(path_batch, 'rb') as file:\n",
    "        _, token_embs, _ = pickle.load(file)\n",
    "\n",
    "    temp = []\n",
    "    for words, embs in zip(words_, token_embs): \n",
    "        if words:\n",
    "            idfs = [word2idf[word] if word in word2idf else 0.0 for word in words]\n",
    "            emb = np.dot(embs.T, idfs)\n",
    "        else:\n",
    "            emb = np.full(token_embs[0].shape[1], np.nan)\n",
    "        temp += [emb]\n",
    "    \n",
    "    i_batch = int(path_batch.stem.split('_')[1])\n",
    "    embeddings_w += [(i_batch, temp)]\n",
    "    \n",
    "embeddings_w.sort(key=lambda x: x[0])\n",
    "embeddings_w = [ee for _, e in embeddings_w for ee in e]\n",
    "embeddings_w = np.stack(embeddings_w)\n",
    "\n",
    "with open(path_embw, 'wb') as file:\n",
    "    np.save(file, embeddings_w)\n",
    "\n",
    "with open(path_embw, 'rb') as file:\n",
    "    embeddings_w = np.load(file)\n",
    "    \n",
    "print(embeddings_w.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
